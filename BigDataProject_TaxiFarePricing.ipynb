{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup\n",
    "\n",
    "Installed the pip installer and anaconda2 with Python 2.7 (for PySpark compatibility reasons). Added the NumPy and Python Kafka module onto the existing version of Python through the pip installer. Connected Anaconda and Jupyter to the PySpark environment setting certain paths and variables: \n",
    "\n",
    "export SPARK_HOME=/usr/lib/spark |\n",
    "export PATH=$SPARK_HOME/bin:$PATH |\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter |\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook' |\n",
    "export PYSPARK_PYTHON=/home/cloudera/anaconda3/envs/python2/bin/python2.7\n",
    "\n",
    "Subsequently Jupyter is launchable through PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Data Pipeline: Data Preparation\n",
    "\n",
    "Since the used data does not originate from a real-time stream, past data files have been downloaded from http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml. The obtained CSV-files need to be places on the Desktop in a folder called 'Data'. Once this step completed there is no further directory structure to be added.\n",
    "\n",
    "The following script pre-slices the files into smaller CSV-files, which each cover about 5 minutes of data inside the files. Since the yellow cab taxi service has a higher ride frequency, it needs to be adjusted for in the slicing of records to achieve a similar pick up time frame per slice. The slicing of a green Taxi data set took around 1 minute compared to a yellow Taxi data set during testing. Later on, the newly generated CSV-data chunks will be picked up again and used for Kafka messaging.\n",
    "\n",
    "This script only needs to be exeuted once and can be re-used after new data sets appear every month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess as sp\n",
    "from os import listdir, path, makedirs\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "class FileSlicer:\n",
    "    \"\"\"\n",
    "    This class is responsible for slicing the CSV-files containing the taxi ride records\n",
    "    into smaller elements, containing around 5 minutes of data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Setup of the class structure and execution of the essential class methods, to split the\n",
    "        data sheets into smaller chunks.\n",
    "        \"\"\"\n",
    "        self.filesList = []\n",
    "        self.get_files()\n",
    "        self.split_data()\n",
    "\n",
    "    def get_files(self):\n",
    "        \"\"\"\n",
    "        Creation of a list of all files directory location contained in the Desktop's 'Data'-folder\n",
    "        for iteration purposes.\n",
    "        \"\"\"\n",
    "        filesList = [f for f in listdir('/home/cloudera/Desktop/Data')\n",
    "                     if isfile(join('/home/cloudera/Desktop/Data', f)) and '.csv' in f]\n",
    "\n",
    "        for i in filesList:\n",
    "            splitString = i.split('_')\n",
    "            splitDate = splitString[2].split('-')\n",
    "            self.filesList.append(['/home/cloudera/Desktop/Data/' + i,\n",
    "                                   splitString[0], splitDate[0], splitDate[1][:2]])\n",
    "\n",
    "    def split_data(self):\n",
    "        for i in self.filesList:\n",
    "            size = 0\n",
    "            \"\"\"\n",
    "            Adjustment for the different data sets. Preparation of the iteration for the slicing\n",
    "            of the data.\n",
    "            \"\"\"\n",
    "            if i[1]  == \"yellow\":\n",
    "                size = 1000\n",
    "            elif i[1] == \"green\":\n",
    "                size = 100\n",
    "            rows = pd.read_csv(i[0], header = 'infer', chunksize = size)\n",
    "            self.slice_output(rows, i[1], i[2], i[3])\n",
    "\n",
    "    def slice_output(self, data, color, year, month):\n",
    "        \"\"\"\n",
    "        Actual slicing of the files into smaller chunks and placement of just those into a new folder\n",
    "        'Chunks' on the Desktop, with each of the chunks which once belonged to the same file being\n",
    "        in the same subfolder named after the originating file.\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        newpath = '/home/cloudera/Desktop/Chunks/' \\\n",
    "                  + color + \"_\" + year + \"_\" + month\n",
    "        if not path.exists(newpath):\n",
    "            makedirs(newpath)\n",
    "        for chunk in data:\n",
    "            chunk.to_csv(newpath + '/_' + str(counter) + \".csv\", sep = '\\t')\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    slicer = FileSlicer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Data Pipeline: Data Slices to KafkaProducer\n",
    "\n",
    "In the following script the generated data chunks will be picked up from the originally created Desktop's directory 'Chunks'. The operation of picking up a data chunk and sending it through a Kafka Producer is executed in batches every five minutes to simulate a real-time data stream and correspond to the time frame contained in each data chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from kafka import SimpleProducer, KafkaClient\n",
    "from os import listdir, path, makedirs\n",
    "from pyspark.sql import SQLContext\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime\n",
    "import Tkinter, tkFileDialog\n",
    "import pandas as pd\n",
    "import sched, time\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "class Streamer:\n",
    "    \"\"\"\n",
    "    Execution of a script every five minutes, as this approximately the time frame\n",
    "    of data every data chunk contains.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "       \"\"\"\n",
    "       Setting-up the instance variables and calling on the initiation method.\n",
    "       \"\"\"\n",
    "       self.filesList = []\n",
    "       self.chunk_counter = 0\n",
    "       self.kafka = KafkaClient(\"localhost:9092\")\n",
    "       self.producer = SimpleProducer(self.kafka)\n",
    "       self.setup()\n",
    "    \n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Selection of the corresponding original file, which is supposed to be streamed\n",
    "        and retraction, sorting and location of all data chunks contained in the selected\n",
    "        folder.\n",
    "        \"\"\"\n",
    "        root = Tkinter.Tk()\n",
    "        root.withdraw()\n",
    "        self.file_path = tkFileDialog.askdirectory()\n",
    "        \n",
    "        filesList = [f for f in listdir(self.file_path)\n",
    "                     if isfile(join(self.file_path, f)) and '.csv' in f]\n",
    "        \n",
    "        numberList = []\n",
    "        \n",
    "        for i in filesList:\n",
    "            numberList.append(i[i.find(\"_\")+1:i.find(\".\")])\n",
    "        \n",
    "        numberList.sort(key=int)\n",
    "        \n",
    "        for i in numberList:\n",
    "            self.filesList.append(self.file_path+\"/_\"+i+\".csv\")\n",
    "            \n",
    "        if \"yellow\" in str(self.file_path):\n",
    "            self.chunk_size = 1000\n",
    "        else:\n",
    "            self.chunk_size = 100\n",
    "            \n",
    "        filesList, numberList = [], []\n",
    "        \n",
    "    def kafkaStream(self):\n",
    "        \"\"\"\n",
    "        Reading of the next data chunk .csv-file and conversion into a Pandas-dataframe.\n",
    "        Extraction of the column headers and all column data, which are stored into a\n",
    "        respective data structures. Zipping of the header with each data line into a \n",
    "        dictionary in order to easily convert it to a .json-file. The generated file\n",
    "        is streamed through the Kafka Producer which is established subsequently.\n",
    "        \"\"\"\n",
    "        if self.chunk_counter >= len(self.filesList):\n",
    "            tkMessageBox.showinfo(\"Streaming completed\", \"The streaming of the file (%s)\" +\n",
    "                         \"has been completed.\" % self.file_path)\n",
    "            sys.exit()\n",
    "        \n",
    "        rawFile = pd.read_csv(self.filesList[self.chunk_counter], \n",
    "                              header = 'infer', chunksize = 1)\n",
    "        \n",
    "        dataframe = rawFile.get_chunk(self.chunk_size)\n",
    "\n",
    "        headerPrep = str(list(dataframe.axes)[1])\n",
    "        header = headerPrep[headerPrep.find(\"[\")+4:headerPrep.find(\"]\")-1].split('\\\\')\n",
    "\n",
    "        data = dataframe.values\n",
    "        datalist = []\n",
    "\n",
    "        for i in data:\n",
    "            raw = str(i)[str(i).find(\"[\")+3:str(i).find(\"]\")-1]\n",
    "            datalist.append(raw.split('\\\\'))\n",
    "\n",
    "        for i in datalist:\n",
    "            raw_message = dict(zip(header, datalist[0]))\n",
    "            message = json.dumps(raw_message)\n",
    "            self.producer.send_messages(\"TaxiData\", message)\n",
    "        \n",
    "        self.chunk_counter += 1 \n",
    "             \n",
    "\n",
    "def automate_kafkaProducer(batch_input): \n",
    "    \"\"\"\n",
    "    Calling on the respective method inside the Streamer()-Class in order to send the\n",
    "    data through the Kafka Producer. This is followed by a latency of 5 minutes until\n",
    "    the next call of the method is executed to simulate a real-time data stream.\n",
    "    \"\"\"\n",
    "    batch_start_time = datetime.now()\n",
    "    stream.kafkaStream()\n",
    "    batch_end_time = datetime.now()\n",
    "    batch_duration = ((batch_end_time - batch_start_time).total_seconds())\n",
    "    batch.enter(300-batch_duration, 1, automate_kafkaProducer, (batch_input,))\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    stream = Streamer()\n",
    "    batch = sched.scheduler(time.time, time.sleep)    \n",
    "    batch.enter(0, 1, automate_kafkaProducer, (batch,))\n",
    "    batch.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receiving Data with the Kafka Consumer\n",
    "\n",
    "The messages sent by the Kafka Producer are received inside Spark using PySpark setting up a Kafka Consumer. The data rows subsequently read into the Spark data frame and saved onto HDFS. The datagrame is reduced by unnecessary data columns and further changes are made to the data labeling. Finally the dataframe is parsed to a Pandas dataframe for further analysis in the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import hour, expr\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SQLContext\n",
    "import json\n",
    "\n",
    "#========================================================================================================#\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "#1)\n",
    "#If you have spark 2.2+ structured streaming is supported, so the kafka stream can directly be consumed as a Spark Data Frame\n",
    "# Construct a streaming DataFrame that reads from topic1\n",
    "df = spark.readStream \\\n",
    "  .format(\"kafka\").option(\"kafka.bootstrap.servers\", \"quickstart.cloudera:9092\").option(\"subscribe\", \"TaxiData\")\\\n",
    "  .option(\"startingOffsets\", \"earliest\").load()\n",
    "\n",
    "#2)\n",
    "#This would have been the approach if we implemented a Spark Streaming consumer - also uncomment the Streaming Context!\n",
    "kafkastream = KafkaUtils.createDirectStream(ssc, topics = ['TaxiData'], \n",
    "                                        kafkaParams = {\"metadata.broker.list\": 'quickstart.cloudera:9092'})\n",
    "\n",
    "    \n",
    "#3)\n",
    "#This is the KafkaConsumer using the kafka library - uncomment all following lines to see the messages appear in the jupyter window \n",
    "consumer = KafkaConsumer('TaxiData', fetch_max_bytes = 2048, auto_offset_reset = 'earliest', enable_auto_commit=False, bootstrap_servers=['quickstart.cloudera:9092'])\n",
    "\n",
    "#for m in consumer:\n",
    "  #  print(m)\n",
    "\n",
    "        \n",
    "#We could not store the received data into dataframes \n",
    "#Performance issues made it hard to test codes on incoming data\n",
    "\n",
    "df = sqlContext.read.json(\"m\")\n",
    "#Stream to HDFS\n",
    "df.write.format(\"csv\").save(\"/TaxiData/\"+counter+\".csv\")\n",
    " \n",
    "#============================================================================================================#\n",
    "#df = sqlContext.read.json(\"TEST2.csv\") - use this command if you work with json files\n",
    "\n",
    "\n",
    "#Start pyspark with the following command: $ pyspark --packages com.databricks:spark-csv_2.11:1.4.0\n",
    "df = sqlContext.read.load('_0.csv', format='com.databricks.spark.csv', header='true', inferSchema='true')\n",
    "df.dtypes - this command checks the types of the inferred schema - check if the datatypes are correct\n",
    "#make sure tpep_pickup_datetime is a timestamp!\n",
    "\n",
    "\n",
    "df = (df.withColumnRenamed('PULocationID', 'pu_id').withColumnRenamed('DOLocationID', 'do_id'))\n",
    "#df.withColumn('tpep_pickup_datetime', df.pickup_datetime.cast('timestamp'))\n",
    "#use the last line if you have to cast the datatype\n",
    "\n",
    "#this is the data relevant for modelling; The hour from datetime is extracted, as we assume this one relevant \n",
    "#for possible delay in traffic\n",
    "df_selection = df.select('pu_id', 'do_id', 'trip_distance', 'total_amount', hour('tpep_pickup_datetime').alias('hour'))            \n",
    "\n",
    "#The reduced data is transformed into a pandas dataframe for further modelling\n",
    "df_pandas = df_selection.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Model\n",
    "\n",
    "The data model is implemented using the Pandas data frame as well as the Scikit-learn library. Firstly, a dummy variable is created based on the suspected rush hour times, in order to evaluate how strong it will affect future fare pricing. Dependent and independent variables are assigned and test as well as train data sets are defined.Finally linear regressions using Scikit are executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "#extract the hour values and transform them into a dummy for rushhour and night-shift\n",
    "hour_values = df_pandas[\"hour\"].values\n",
    "\n",
    "rush_hour_dummy = []\n",
    "for i in hour_values: \n",
    "    if i >= 16 and i < 20 or i >= 8 and i < 10 or i >= 0 and i < 6:\n",
    "        rush_hour_dummy.append(1)\n",
    "    else:\n",
    "        rush_hour_dummy.append(0)\n",
    "\n",
    "series_dummy = pd.Series(rush_hour_dummy, name='hourdummy')\n",
    "\n",
    "#the set of independent variables is saved in df_X and the price we want to estimate in df_Y\n",
    "df_pandas2 = pd.concat([df_pandas, series_dummy], axis=1)\n",
    "\n",
    "df_X = df_pandas2[[\"trip_distance\",\"hourdummy\"]]\n",
    "df_Y = df_pandas[[\"total_amount\"]]\n",
    "\n",
    "\n",
    "#Split the data into training/testing sets\n",
    "#source: http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n",
    "X_train = df_X[:-100]\n",
    "X_test = df_X[-100:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "Y_train = df_Y[:-100]\n",
    "Y_test = df_Y[-100:]\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train,Y_train )\n",
    "print('Coefficients: \\n', lr.coef_)\n",
    "\n",
    "#predicts the Y-values using the testing set\n",
    "Y_pred = lr.predict(X_test)\n",
    "\n",
    "# The mean squared error - compares the predicted y-values with the real values of the test-data\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(Y_test, Y_pred))\n",
    "# Explained variance score (coefficient of determination): 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# User Interface\n",
    "\n",
    "We implemented a simple user interface using tkinter..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put interface in here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
